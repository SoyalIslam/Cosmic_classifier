ğŸš€ Cosmic Classifier Challenge Report

ğŸŒŒ Theme

The Galactic Classification Challenge (GCC) is a futuristic machine learning competition where participants must decode Dr. Klaus Reinhardtâ€™s final transmission. The objective is to classify planets based on their survival potential and resource availability using machine learning techniques.

ğŸ›°ï¸ Introduction

In the year 2547, Dr. Klaus Reinhardt, a renowned scientist and space explorer, made an extraordinary discovery while exploring a distant galaxy. He developed a system to classify planets based on their habitability and resource potential. However, before he could fully transmit his findings to Earth, his ship was caught in a black holeâ€™s gravitational field. His final transmission, containing valuable planetary classification data, was corrupted due to gravitational interference, introducing missing values and noise.

ğŸŒ Humanityâ€™s survival depends on correctly classifying these planets using the available data.

ğŸ” Problem Statement

Participants are given a dataset containing planetary attributes and must develop a classification model to assign each planet to one of ten planetary classes. The dataset is partially corrupted, requiring participants to handle missing values and noise efficiently.

ğŸ“Š Dataset Details

The dataset consists of 10 input features and 10 output classes, with data irregularities such as missing values marked by large negative numbers.

ğŸŒ Input Features (10 planetary attributes)

Atmospheric Density (kg/mÂ³) â€“ Thickness of the atmosphere.

Surface Temperature (Kelvin) â€“ Average temperature of the planetâ€™s surface.

Gravity (m/sÂ²) â€“ Surface gravity of the planet.

Water Content (%) â€“ Percentage of surface covered by water.

Mineral Abundance (0-1) â€“ Availability of valuable minerals.

Orbital Period (Earth days) â€“ Time taken to orbit its star.

Proximity to Star (AU) â€“ Distance from its parent star.

Magnetic Field Strength (Tesla) â€“ Strength of the planetâ€™s magnetic field.

Radiation Levels (Sieverts/year) â€“ Average surface radiation exposure.

Atmospheric Composition Index (0-1) â€“ Suitability of the atmosphere for human life.

ğŸª Output Classes (10 planetary types)

ğŸŒ Bewohnbar

ğŸ— Terraformierbar

â› Rohstoffreich

ğŸ”¬ Wissenschaftlich

â˜ Gasriese

ğŸœ WÃ¼stenplanet

â„ Eiswelt

â˜  ToxischetmosÃ¤re

âš¡ Hohestrahlung

ğŸ’€ Toterahswelt

ğŸ¯ Project Goals

The primary objective is to develop a robust machine learning classification model to accurately categorize planets based on their features. Given the noisy nature of the dataset, preprocessing steps such as handling missing values, feature scaling, and dimensionality reduction are necessary.

ğŸ›  Methodology

1ï¸âƒ£ Data Preprocessing

Handling Missing Values: Identified missing values (-999 or other outliers) and replaced them using mean/median imputation.

Feature Scaling: Normalized numerical features to ensure uniformity.

Outlier Detection: Applied statistical techniques to detect and remove anomalies.

Feature Engineering: Created additional relevant features where necessary.

2ï¸âƒ£ Model Selection

We experimented with multiple machine learning models:

ğŸ¤– Logistic Regression

ğŸŒ² Random Forest Classifier

ğŸ”¥ Gradient Boosting (XGBoost, LightGBM, CatBoost)

ğŸ“ˆ Support Vector Machines (SVM)

ğŸ§  Neural Networks (MLPClassifier from sklearn)

3ï¸âƒ£ Model Training and Evaluation

Split the data into training (80%) and validation (20%) sets.

Tuned hyperparameters using GridSearchCV and RandomizedSearchCV.

Evaluated models using accuracy, precision, recall, and F1-score.

4ï¸âƒ£ Final Model Selection

After comparing multiple models, we selected XGBoost, which achieved the highest accuracy on the validation set.

ğŸ“ˆ Results and Analysis

ğŸ† Best Model Accuracy: XGBoost with 91.5% validation accuracy

ğŸ“Š Confusion Matrix Analysis: Showed some misclassifications in similar planet categories.

ğŸ“Œ Feature Importance: Gravity, Atmospheric Density, and Water Content were the most critical features.

ğŸš€ Model Explanation: XGBoost

XGBoost (Extreme Gradient Boosting) is a powerful ensemble learning algorithm that builds multiple decision trees iteratively, minimizing errors at each step.

âœ… Key Strengths of XGBoost:

ğŸ”„ Boosting Mechanism â€“ Improves weak learners sequentially, reducing bias.

â“ Handling Missing Values â€“ Automatically determines optimal splits even with missing data.

ğŸ“Š Feature Importance Analysis â€“ Identifies which features have the most impact on classification.

ğŸ›‘ Regularization (L1 & L2) â€“ Prevents overfitting by penalizing complex models.

âš¡ Parallel Processing â€“ Enhances computational efficiency, making it suitable for large datasets.

ğŸ›  Hyperparameter Tuning for XGBoost:

Parameter

Value

n_estimators

200

max_depth

6

learning_rate

0.1

subsample

0.8

colsample_bytree

0.8

ğŸ“Œ Conclusion

The Galactic Classification Challenge successfully demonstrated the power of machine learning in planetary classification. XGBoost emerged as the best-performing model with an impressive 91.5% validation accuracy. Future improvements could include deep learning techniques and advanced feature engineering.

ğŸŒŸ This project takes humanity one step closer to interstellar exploration! ğŸš€

ğŸ‘¥ Team Details

ğŸ§‘â€ğŸš€ Tapas Pandey: https://github.com/Tapas-Pandey

ğŸ›° Kartike Veram: https://github.com/kartikeVr

ğŸ“Š Soyal Islam: https://github.com/SoyalIslam

ğŸ’» Dev Saini: https://github.com/devsaini889

ğŸ¨ Khushi Singh: https://github.com/khushisingh14

ğŸ“· Visuals









